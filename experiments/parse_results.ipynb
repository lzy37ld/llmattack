{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Strings Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'gcg'\n",
    "logdir = f'results/'\n",
    "\n",
    "# for individual experiments\n",
    "individual = True\n",
    "mode = 'behaviors'\n",
    "\n",
    "files = !ls {logdir}individual_{mode}_*_ascii*\n",
    "files = [f for f in files if 'json' in f]\n",
    "files = sorted(files, key=lambda x: \"_\".join(x.split('_')[:-1]))\n",
    "\n",
    "max_examples = 100\n",
    "\n",
    "logs = []\n",
    "for logfile in files:\n",
    "    with open(logfile, 'r') as f:\n",
    "        logs.append(json.load(f))\n",
    "log = logs[0]\n",
    "len(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = log['params']\n",
    "print(config.keys())\n",
    "\n",
    "total_steps = config['n_steps']\n",
    "test_steps = config.get('test_steps', 50)\n",
    "log_steps = total_steps // test_steps + 1\n",
    "print('log_steps', log_steps)\n",
    "\n",
    "if individual:\n",
    "    examples = 0\n",
    "    test_logs = []\n",
    "    control_logs = []\n",
    "    goals, targets = [],[]\n",
    "    for l in logs:\n",
    "        sub_test_logs = l['tests']\n",
    "        sub_examples = len(sub_test_logs) // log_steps\n",
    "        examples += sub_examples\n",
    "        test_logs.extend(sub_test_logs[:sub_examples * log_steps])\n",
    "        control_logs.extend(l['controls'][:sub_examples * log_steps])\n",
    "        goals.extend(l['params']['goals'][:sub_examples])\n",
    "        targets.extend(l['params']['targets'][:sub_examples])\n",
    "        if examples >= max_examples:\n",
    "            break\n",
    "else:\n",
    "    test_logs = log['tests']\n",
    "    examples = 1\n",
    "\n",
    "passed, em, loss, total = [],[],[],[]\n",
    "for i in range(examples):\n",
    "    sub_passed, sub_em, sub_loss, sub_total = [],[],[],[]\n",
    "    for res in test_logs[i*log_steps:(i+1)*log_steps]:\n",
    "        sub_passed.append(res['n_passed'])\n",
    "        sub_em.append(res['n_em'])\n",
    "        sub_loss.append(res['n_loss'])\n",
    "        sub_total.append(res['total'])\n",
    "    passed.append(sub_passed)\n",
    "    em.append(sub_em)\n",
    "    loss.append(sub_loss)\n",
    "    total.append(sub_total)\n",
    "passed = np.array(passed)\n",
    "em = np.array(em)\n",
    "loss = np.array(loss)\n",
    "total = np.array(total)\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em[...,0].mean(0)[-1], loss[...,0].mean(0)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Behaviors Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(To get more accurate results, please run the cells below, then use `evaluate_individual.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ls: cannot access 'results/individual_llama2_behaviors': No such file or directory\"]\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(logfile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m         logs\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mload(f))\n\u001b[0;32m---> 21\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mlogs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mlen\u001b[39m(logs)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# method = 'gcg'\n",
    "# logdir = f'results/'\n",
    "\n",
    "# # for individual experiments\n",
    "# individual = True\n",
    "# mode = 'behaviors'\n",
    "# model = \"llama2\"\n",
    "\n",
    "# files = !ls {logdir}individual_{model}_{mode}\n",
    "# print(files)\n",
    "# files = [f for f in files if 'json' in f]\n",
    "# files = sorted(files, key=lambda x: \"_\".join(x.split('_')[:-1]))\n",
    "\n",
    "# max_examples = 100\n",
    "# print(files)\n",
    "\n",
    "# logs = []\n",
    "# for logfile in files:\n",
    "#     with open(logfile, 'r') as f:\n",
    "#         logs.append(json.load(f))\n",
    "# log = logs[0]\n",
    "# len(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "individual = True\n",
    "root = \"/home/liao.629/llm-attacks/experiments/results\"\n",
    "pattern = re.compile(r\"individual_behaviors_llama2_gcg.*json\")\n",
    "\n",
    "x = [path for path in os.listdir(root) if pattern.match(path)]\n",
    "\n",
    "logs = []\n",
    "for logfile in x:\n",
    "    with open(os.path.join(root,logfile), 'r') as f:\n",
    "        logs.append(json.load(f))\n",
    "log = logs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['goals', 'targets', 'test_goals', 'test_targets', 'control_init', 'test_prefixes', 'models', 'test_models', 'n_steps', 'test_steps', 'batch_size', 'topk', 'temp', 'allow_non_ascii', 'target_weight', 'control_weight', 'anneal', 'incr_control', 'stop_on_success'])\n",
      "log_steps 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = log['params']\n",
    "print(config.keys())\n",
    "\n",
    "total_steps = config['n_steps']\n",
    "test_steps = config.get('test_steps', 50)\n",
    "log_steps = total_steps // test_steps + 1\n",
    "print('log_steps', log_steps)\n",
    "\n",
    "if individual:\n",
    "    examples = 0\n",
    "    test_logs = []\n",
    "    control_logs = []\n",
    "    goals, targets = [],[]\n",
    "    for l in logs:\n",
    "        sub_test_logs = l['tests']\n",
    "        sub_examples = len(sub_test_logs) // log_steps\n",
    "        examples += sub_examples\n",
    "        test_logs.extend(sub_test_logs[:sub_examples * log_steps])\n",
    "        control_logs.extend(l['controls'][:sub_examples * log_steps])\n",
    "        goals.extend(l['params']['goals'][:sub_examples])\n",
    "        targets.extend(l['params']['targets'][:sub_examples])\n",
    "        # if examples >= max_examples:\n",
    "        #     break\n",
    "else:\n",
    "    test_logs = log['tests']\n",
    "    examples = 1\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_test_logs = logs[0][\"tests\"]\n",
    "len(sub_test_logs) // log_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1560"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(control_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 3, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "passed, em, loss, total, controls = [],[],[],[],[]\n",
    "for i in range(examples):\n",
    "    sub_passed, sub_em, sub_loss, sub_total, sub_control = [],[],[],[],[]\n",
    "    for res in test_logs[i*log_steps:(i+1)*log_steps]:\n",
    "        sub_passed.append(res['n_passed'])\n",
    "        sub_em.append(res['n_em'])\n",
    "        sub_loss.append(res['n_loss'])\n",
    "        sub_total.append(res['total'])\n",
    "    sub_control = control_logs[i*log_steps:(i+1)*log_steps]\n",
    "    passed.append(sub_passed)\n",
    "    em.append(sub_em)\n",
    "    loss.append(sub_loss)\n",
    "    total.append(sub_total)\n",
    "    controls.append(sub_control)\n",
    "passed = np.array(passed)\n",
    "em = np.array(em)\n",
    "loss = np.array(loss)\n",
    "total = np.array(total)\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saved_controls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msaved_controls\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saved_controls' is not defined"
     ]
    }
   ],
   "source": [
    "saved_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 3, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_controls = [c[-1] for c in controls]\n",
    "json_obj = {\n",
    "    'goal': goals,\n",
    "    'target': targets,\n",
    "    'controls': saved_controls\n",
    "}\n",
    "with open('results/individual_behavior_controls.json', 'w') as f:\n",
    "    json.dump(json_obj, f)\n",
    "\n",
    "# now run `evaluate_individual.py` with this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saved_controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'eval/individual_behavior_controls.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval/individual_behavior_controls.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eval/individual_behavior_controls.json'"
     ]
    }
   ],
   "source": [
    "data = json.load(open('eval/individual_behavior_controls.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(data['Vicuna-7B']['jb']) == 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `evaluate.py` on the logfile first to generate a log in the eval dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log(log, jb, idx=-1):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
    "\n",
    "    # Plotting the bars in the first plot\n",
    "    bars = axes[0].bar(log.keys(), jb[:, idx])\n",
    "    axes[0].xaxis.set_tick_params(rotation=90)\n",
    "    axes[0].grid(axis='y', ls='dashed')\n",
    "\n",
    "    # Plotting the lines in the second plot\n",
    "    lines = []\n",
    "    for i in range(len(log)):\n",
    "        line, = axes[1].plot(range(len(jb[0])), jb[i], label=list(log.keys())[i])\n",
    "        lines.append(line)\n",
    "\n",
    "    # Getting the handles and labels from the legend of the second plot\n",
    "    handles, labels = axes[1].get_legend_handles_labels()\n",
    "\n",
    "    # Plotting the legend in the first plot using the handles and labels from the second plot\n",
    "    axes[0].legend(handles=handles, labels=labels, bbox_to_anchor=(1.1, -0.45, 2., .102),\n",
    "                loc='lower left', ncol=4, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    axes[2].plot(range(len(jb[0])), jb.mean(0), color='red')\n",
    "    axes[2].set_ylim(0, 100)\n",
    "    axes[2].grid(axis='y', ls='dashed')\n",
    "\n",
    "    # Matching the colors of the bars in the first plot with the lines in the legend\n",
    "    for bar, line in zip(bars, lines):\n",
    "        bar.set_color(line.get_color())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = f'eval/'\n",
    "logfile = <your_logfile>\n",
    "\n",
    "with open(logdir + logfile, 'r') as f:\n",
    "    log = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb, em = [],[]\n",
    "for model in log:\n",
    "    stats = log[model]\n",
    "    jb.append(stats['test_jb'])\n",
    "    em.append(stats['test_em'])\n",
    "jb = np.array(jb)\n",
    "jb = jb.mean(-1)\n",
    "em = np.array(em)\n",
    "em = em.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log(log, jb, idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
